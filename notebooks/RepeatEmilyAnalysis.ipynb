{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuraltda.topology2 as tp2\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import neuraltda.simpComp as sc\n",
    "import neuraltda.topology2 as tp2\n",
    "import neuraltda.spectralAnalysis as sa\n",
    "import datetime\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "from ephys import core, events, clust\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rebin with 1ms, no overlap, start+2500ms to end+500ms, correct trials\n",
    "block_path = ''\n",
    "winsize = 1.0\n",
    "dtover = 0.0\n",
    "segment = [2500, 500]\n",
    "\n",
    "cluster_group = ['Good', 'MUA']\n",
    "widenarrow_threshold = 0.000230 # sw threshold in seconds\n",
    "\n",
    "spikes = core.load_spikes(block_path)\n",
    "trials = events.load_trials(block_path)\n",
    "fs = core.load_fs(block_path)\n",
    "\n",
    "# Get wide/narrow clusters\n",
    "clusters = core.load_clusters(block_path)\n",
    "clusters_list = clusters[clusters.quality.isin(cluster_group)]['cluster'].unique()\n",
    "(wide, narrow) = clust.get_wide_narrow(block_path, clusters_list, widenarrow_threshold)\n",
    "\n",
    "# Get Correct/Incorrect Trials\n",
    "correctTrials = trials[trials['correct']==True]\n",
    "incorrectTrials = trials[trials['correct']==False]\n",
    "\n",
    "# Bin and \n",
    "bfdict = tp2.do_dag_bin_lazy(block_path, spikes, correctTrials, clusters, fs, winsize,\n",
    "                                    segment, cluster_group=['Good', 'MUA'],\n",
    "                                    dt_overlap=dtover, comment='SD-emily-reanalysis')\n",
    "bdf = glob.glob(os.path.join(bfdict['raw'], '*.binned'))[0]\n",
    "print('BDF: {}'.format(bdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# contexts:\n",
    "contexts = {\n",
    "    'A_hi': 'T40S40D3',\n",
    "    'A_lo': 'T3S3D40',\n",
    "    'AS_hi': 'T40S40D-1',\n",
    "    'AS_lo': 'T3S3D-1'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute average psths\n",
    "avg_psths = {}\n",
    "with h5.File(bdf, 'r') as binned_data:\n",
    "    for context in contexts.keys():\n",
    "        stim = contexts[context]\n",
    "        fulldata = np.array(binned_data[stim]['pop_tens'])\n",
    "        avg_psth = np.mean(fulldata, axis=2)\n",
    "        avg_psths[str(context)] = avg_psth\n",
    "    binned_clusters = binned_data[stim]['clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute MSEs:\n",
    "\n",
    "def mse(psth_a, psth_b, psth_c, psth_d):\n",
    "    \n",
    "    mse_ab = np.sum(np.square(psth_a - psth_b), axis=1)\n",
    "    mse_cd = np.sum(np.square(psth_c - psth_d), axis=1)\n",
    "    return mse_ab + mse_cd\n",
    "\n",
    "MSE_targ = mse(avg_psths['A_hi'], avg_psths['AS_hi'], avg_psths['A_lo'], avg_psths['AS_lo'])\n",
    "MSE_dist = mse(avg_psths['A_hi'], avg_psths['AS_lo'], avg_psths['A_lo'], avg_psths['AS_hi'])\n",
    "MSE_diff = MSE_dist - MSE_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Break into wide/narrow\n",
    "wide_mse_diff = MSE_diff[np.in1d(binned_clusters, wide)]\n",
    "narrow_mse_diff = MSE_diff[np.in1d(binned_clusters, narrow)]\n",
    "\n",
    "#Print wide\n",
    "print('Wide Clusters')\n",
    "for ind, val in enumerate(wide_mse_diff):\n",
    "    print(\"Cluster: {}    MSE Difference: {:10.4f}\".format(ind, val))\n",
    "    \n",
    "#Print Narrow\n",
    "print('Narrow Clusters')\n",
    "for ind, val in enumerate(narrow_mse_diff):\n",
    "    print(\"Cluster: {}    MSE Difference: {:10.4f}\".format(ind, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_h = np.random.standard_normal((20, 100))\n",
    "as_h = np.random.standard_normal((20, 100))\n",
    "a_l = np.random.standard_normal((20, 100))\n",
    "as_l = np.random.standard_normal((20, 100))\n",
    "\n",
    "mse_t = mse(a_h, as_h, a_l, as_l)\n",
    "mse_d = mse(a_h, as_l, a_l, as_h)\n",
    "mse_diff = mse_d - mse_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-20.25190463,  60.57024706, -38.27126792, -39.20822378,\n",
       "        34.22506067, -21.56016953, -17.54711924,  56.44565946,\n",
       "       -21.48444559,  -7.82825565,  89.79055453,  15.06173692,\n",
       "        24.89982774, -17.02510418, -24.18807837,  52.66583291,\n",
       "       -32.68713435,  48.00001554, -43.18510708,  11.15946077])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
